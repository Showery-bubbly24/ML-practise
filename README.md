# ML-practise
profiki

# Парсинг и предобработка данных

## Загружены данные из предоставленных ресурсов
- Здесь все просто, загружаем и работаем.
## Используются данные из дополнительных источников
- Искать дополнительный датасет в сети можно эффективно, если следовать ряду правил и использовать правильные инструменты. Вот несколько ключевых советов:

**1. Определите цели поиска**
- Четко сформулируйте, какой именно дополнительный датасет вам нужен (например, данные о погоде, демографии, финансовые данные).
- Сопоставьте формат данных основного датасета с возможными дополнительными (структура, временные рамки, географическая область и т.д.).

**2. Используйте специализированные поисковые ресурсы**
**Порталы с открытыми данными**: 
- Kaggle
- UCI Machine Learning Repository
- Google Dataset Search
- Официальные правительственные порталы (например, data.gov или data.gov.ru).
- Узкоспециализированные репозитории для вашего домена: медицинские, финансовые, транспортные данные и т.д.

**3. Уточняйте запросы в поисковике**
Используйте ключевые слова, такие как: 
- dataset
- open data
- csv/json/excel
- Конкретная область, например: "healthcare dataset 2023 csv".
- Фильтруйте по дате публикации, чтобы найти актуальные данные.

**4. Проверяйте источники**

Обратите внимание на авторитетность источника (государственные сайты, крупные исследовательские институты, университеты).

Убедитесь, что данные лицензированы для использования.

**5. Ищите метаданные и сопутствующую информацию**

Часто метаданные из основного датасета содержат ссылки на сопутствующие данные или информацию об их происхождении.

**6. Используйте API для интеграции данных**

Если данные должны быть динамическими, поищите сервисы с API. Например: 

OpenWeatherMap для данных о погоде.

Twitter API для анализа социальных сетей.

Quandl или Yahoo Finance для финансовых данных.

**7. Автоматизируйте поиск и проверки**

Используйте скрипты на Python (например, с библиотеками requests, beautifulsoup4) для автоматизации загрузки или анализа найденных данных.

**Лайфхаки:**

**Ищите похожие исследования**: Научные статьи часто содержат ссылки на использованные датасеты. Используйте Google Scholar или ResearchGate.

**Фильтруйте шум**: Добавляйте минус-слова в запрос (например, dataset healthcare -simulation -example).

**Проверьте GitHub**: Разработчики часто выкладывают данные на своих репозиториях. Ищите по ключевым словам и расширениям файлов (например, filename:*.csv).

Эти шаги помогут быстро найти релевантный датасет, подходящий для вашей задачи.
## База данных создана и находится в актуальном состоянии

Получаем IP User и Password и с помощью небольшого скрипта делаем много всякого
```python
from sqlalchemy import create_engine
import pandas as pd 


engine = create_engine('dialect+driver://username:password@host:port/database',
					   echo=False)
df.to_sql(name='tablename', con=engine, if_exists='replace')
```

## Предусмотрена обработка исключений при загрузке в базу данных

При загрузке данных в базу данных важно учитывать и обрабатывать исключения, чтобы обеспечить стабильность процесса и сохранить данные в целостном состоянии. Вот основные аспекты, которые нужно учитывать:

**1. Основные исключения и ситуации:**
- **Ошибки соединения с базой данных** (например, недоступность базы, таймаут).
- **Нарушения ограничений целостности данных** (например, нарушение уникальности, ограничения внешнего ключа).
- **Неверный формат данных** (например, несоответствие типов, пустые значения в обязательных полях).
- **Ошибки транзакций** (например, сбои во время выполнения нескольких зависимых операций).
- **Конфликты при конкурентном доступе** (например, блокировки записей).

**2. Как учитывать исключения:**
- **Использовать транзакции.**
Транзакции позволяют гарантировать, что либо все операции будут выполнены успешно, либо ни одна из них не изменит данные. Это особенно важно при сложных загрузках.
- **Проверять данные перед загрузкой.**
Валидация формата данных (например, через регулярные выражения или схемы JSON).
Убедиться, что все обязательные поля заполнены и корректны.
- **Логировать ошибки.**
Важно сохранять информацию о возникших исключениях для последующего анализа и устранения причин.
- **Реагировать на разные типы ошибок.**
Например, в случае ошибки подключения можно попытаться повторить операцию, а в случае некорректных данных — пропустить запись.
- **Установить лимиты повторных попыток.**
Избегайте бесконечных циклов повторного выполнения, чтобы не перегружать систему.

**3. Пример обработки исключений:**

На примере Python с использованием библиотеки psycopg2 для работы с PostgreSQL:

```python
import psycopg2 from psycopg2 import sql, OperationalError, IntegrityError import logging 

# Настройка логирования 
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') 
def load_data_to_db(data): 
	"""Функция для загрузки данных в базу данных.""" 
	conn = None 
	try: 
		# Подключение к базе данных 
		conn = psycopg2.connect(
			dbname="your_database", 
			user="your_user",
			password="your_password", 
			host="localhost", 
			port="5432" 
		) 
		
		conn.autocommit = False # Используем транзакции 
		cursor = conn.cursor() 
		
		for record in data: 
		try: 
			# Вставка данных 
			query = sql.SQL("Request...") 
			cursor.execute(query, (record['field1'], record['field2'])) 
		except IntegrityError as e: 
			logging.warning(f"error {record}: {e}") 
			conn.rollback() # Откат транзакции для этой записи 
		except Exception as e: 
			logging.error(f"error {record}: {e}") 
			conn.rollback() 
		else: 
			conn.commit() # Фиксируем успешную операцию 
	except OperationalError as e: 
		logging.critical(f"Ошибка подключения к базе данных: {e}") 
	finally: 
		if conn: 
			conn.close() 
			logging.info("Соединение с базой данных закрыто.") 
```

**4. Дополнительные рекомендации:**
- Если загрузка данных масштабна, разбивайте процесс на батчи (пакеты).
- Обеспечьте атомарность операций при необходимости (например, с помощью флагов загрузки данных).
- Регулярно проверяйте логи и добавляйте мониторинг для выявления сбоев в реальном времени.
- Этот подход поможет минимизировать риски и повысить надежность при работе с базой данных.

## Определены и обоснованы наиболее значимые атрибуты

Определение наиболее значимых атрибутов в датасете — это ключевая задача для понимания данных и повышения эффективности моделей машинного обучения. Вот несколько лучших практик, которые помогут решить эту задачу.

**1. Хорошие практики для определения значимых атрибутов:**
- **1.1. Анализ корреляции:**
	- Используйте корреляционную матрицу для количественных данных, чтобы найти атрибуты, сильно связанные с целевой переменной.
	- Выявляйте мультиколлинеарность между признаками.
- **1.2. Методы визуализации:**
	- Используйте графики (например, pairplot, heatmap) для анализа распределений и взаимосвязей.
	- Применяйте графики распределений для категориальных и числовых признаков.
- **1.3. Статистические тесты:**
	- **ANOVA**: Для сравнения категориальных признаков с числовой целевой переменной.
	- **Chi-Square Test**: Для проверки связи между категориальными признаками и целевой переменной.
- **1.4. Методы машинного обучения:**
	- **Feature Importance** с помощью моделей (например, Random Forest, Gradient Boosting).
	- Регуляризация в линейных моделях (Lasso, Ridge) для оценки значимости.
- **1.5. Выбор признаков на основе удалений:**
	- **Recursive Feature Elimination (RFE):** Постепенное исключение наименее значимых признаков.
- **1.6. Учет взаимодействий:**
	- Выявляйте взаимодействия между признаками с помощью полиномиальных признаков или других методов.

**2. Пример на практике:**
Допустим, у нас есть датасет, в котором мы хотим предсказать, будет ли человек покупать товар (целевая переменная Purchase) на основе демографических и поведенческих данных.
**2.1. Загрузка данных:**
```python
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.feature_selection import SelectKBest, chi2 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score 


# Загрузка данных 
data = pd.read_csv('ecommerce_data.csv') 

# Проверка структуры данных 

print(data.head()) 
print(data.info()) 
```

**2.2. Анализ корреляции:**
```python
# Корреляционная матрица для числовых признаков 

correlation_matrix = data.corr() 
plt.figure(figsize=(10, 8)) 
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm') 
plt.title("Корреляционная матрица") plt.show() 
```
Вывод: Определяем, какие числовые признаки наиболее связаны с Purchase.

**2.3. Chi-Square Test для категориальных данных:**
```python
# Выбор категориальных признаков 

categorical_features = ['Gender', 'Browser', 'Device'] 

# Выделение целевой переменной и преобразование категориальных данных 
X_categorical = pd.get_dummies(data[categorical_features], drop_first=True) 
y = data['Purchase'] 

# Применение Chi-Square Test 
chi2_selector = SelectKBest(chi2, k='all') 
chi2_selector.fit(X_categorical, y) 

# Оценка значимости признаков 
chi2_scores = pd.DataFrame({ 
				'Feature': X_categorical.columns, 
				'Score': chi2_selector.scores_ })
					.sort_values(by='Score', ascending=False) 

print(chi2_scores) 
```

**2.4. Feature Importance с Random Forest:**
```python
# Разделение данных на обучающую и тестовую выборки 
X = pd.get_dummies(data.drop(columns=['Purchase']), drop_first=True) 
y = data['Purchase'] 

X_train, X_test, y_train, y_test = train_test_split(
	X, y, 
	test_size=0.3, 
	random_state=42) 
	
# Обучение модели Random Forest 
rf = RandomForestClassifier(random_state=42) 
rf.fit(X_train, y_train) 

# Оценка значимости признаков 
feature_importances = pd.DataFrame({ 
		'Feature': X.columns, 
		'Importance': rf.feature_importances_ 
	}).sort_values(
				by='Importance', 
				ascending=False) 
				
print(feature_importances) 

# Визуализация значимости признаков 
plt.figure(figsize=(10, 6)) 
sns.barplot(x='Importance', y='Feature', data=feature_importances) 
plt.title("Feature Importance (Random Forest)") 
plt.show() 
```

**2.5. Удаление менее значимых признаков:**
```python
# Оставляем только наиболее значимые признаки 
selected_features = feature_importances[feature_importances['Importance'] > 0.01]['Feature'] 
X_train_selected = X_train[selected_features] 
X_test_selected = X_test[selected_features] 

# Повторное обучение модели 
rf.fit(X_train_selected, y_train) 
y_pred = rf.predict(X_test_selected) 

# Оценка качества модели 
print("Accuracy:", accuracy_score(y_test, y_pred))
```

## Для каждого атрибута присутствует текстовое описание

- ### **Для каждого атрибута присутствует информация о количестве пустых значений**

Составление текстового описания атрибутов (или признаков) — это важный шаг в подготовке данных. Такое описание помогает лучше понимать данные и упрощает их использование другими участниками проекта.

**1. Как правильно составлять описание атрибутов:**
- **1.1. Что включить в описание:**
	- **Название атрибута:** Укажите понятное и лаконичное название.
	- **Тип данных:** Укажите, является ли атрибут числовым (integer, float), категориальным (строка, перечисление) или временным (datetime).
	- **Описание:** Объясните, что означает атрибут, его смысл в контексте задачи.
	- **Единицы измерения:** Если данные числовые, укажите, в чем они измеряются.
	- **Пример значения:** Покажите типичное значение, чтобы лучше представить данные.
	- **Диапазон/ограничения:** Укажите минимальные и максимальные значения, если это применимо.
	- **Примечания:** Например, есть ли пропуски, аномалии или важные нюансы.
- **2. Пример на практике:**
Предположим, у нас есть датасет с информацией о пользователях и их покупках в интернет-магазине.

**Описание атрибутов:*

| Название атрибута | Тип данных  | Описание                              | Единицы измерения         | Пример значения | Диапазон/Ограничения | Примечания           |
| ----------------- | ----------- | ------------------------------------- | ------------------------- | --------------- | -------------------- | -------------------- |
| User_ID           | Integer     | Уникальный идентификатор пользователя | Положительное целое число | 123456789       |                      | Уникальное значение  |
| Age               | Integer     | Возраст пользователя                  | Годы                      | 24              | 0-105                | Пропуски отсутствуют |
| Gender            | Categorical | Пол пользователя                      | Текстовое значение        | Female          | FemaleMale           | Пропуски отсутствуют |

**3. Пример в коде Python:**

Вы можете создать описание в формате Pandas DataFrame для удобного использования в проекте:

```python
import pandas as pd 

# Создаем описание 
attribute_description = pd.DataFrame([ 
	{"Attribute": "User_ID", "Type": "Integer", "Description": "Уникальный идентификатор пользователя.", "Units": "-", "Example": "12345", "Range/Constraints": "Положительное целое число", "Notes": "Уникальное значение."}, 
	{"Attribute": "Age", "Type": "Integer", "Description": "Возраст пользователя.", "Units": "Годы", "Example": "25", "Range/Constraints": "18-99", "Notes": "Значения вне диапазона требуют проверки."}, 
	{"Attribute": "Gender", "Type": "Categorical", "Description": "Пол пользователя.", "Units": "-", "Example": "Male", "Range/Constraints": "Male, Female", "Notes": "Пропуски отсутствуют."}, 
	{"Attribute": "Signup_Date", "Type": "Datetime", "Description": "Дата регистрации пользователя в системе.", "Units": "ISO 8601 (гггг-мм-дд)", "Example": "2023-01-15", "Range/Constraints": "-", "Notes": "Временная зона UTC."}, 
	{"Attribute": "Last_Login", "Type": "Datetime", "Description": "Дата последнего входа пользователя в систему.", "Units": "ISO 8601 (гггг-мм-дд)", "Example": "2025-01-01", "Range/Constraints": "-", "Notes": "Значение null для неактивных пользователей."}, 
	{"Attribute": "Purchase_Amount", "Type": "Float", "Description": "Сумма последней покупки пользователя.", "Units": "USD", "Example": "149.99", "Range/Constraints": "0.01-10000.00", "Notes": "Может содержать пропуски (не покупали)."}, 
	{"Attribute": "Country", "Type": "Categorical", "Description": "Страна проживания пользователя.", "Units": "-", "Example": "USA", "Range/Constraints": "ISO 3166-1 Alpha-3", "Notes": "Требует группировки для малых выборок."}, 
	{"Attribute": "Product_Category", "Type": "Categorical", "Description": "Категория последнего купленного товара.", "Units": "-", "Example": "Electronics", "Range/Constraints": "Electronics, Clothing, ...", "Notes": "Возможны новые категории."} 
]) 

# Вывод описания 
print(attribute_description)
```

## Для каждого атрибута присутствует графический и статистический анализы плотности распределения значений

Для анализа распределения данных по каждому атрибуту в датасете важно сочетать **графические** и **статистические методы**. Это помогает получить полное представление о структуре данных, аномалиях, выбросах и форме распределения.

**Шаги для анализа распределения:**
- **1. Графический анализ**
	Используются графики для визуализации данных:
	- **Гистограммы (Histogram)**: Показывают частотное распределение значений.
	- **Кернельная оценка плотности (KDE)**: Гладкая кривая, которая отражает распределение.
	- **Boxplot (ящиковая диаграмма)**: Для выявления медианы, квартилей и выбросов.
	- **Violin Plot**: Сочетание KDE и boxplot.
	- **QQ-Plot (Квантиль-квантиль график)**: Сравнение с нормальным распределением.

- **2. Статистический анализ**
	Используются числовые характеристики и тесты:
	- **Меры центральной тенденции**: Среднее, медиана, мода.
	- **Меры разброса**: Дисперсия, стандартное отклонение, интерквартильный размах.
	- **Тесты на нормальность**: 
		- Шапиро-Уилка (Shapiro-Wilk).
		- Колмогорова-Смирнова.
		- Андерсона-Дарлинга.
	- **Анализ выбросов**: Z-оценка, IQR (межквартильный размах).

- **3. Хорошие практики анализа:**
	**Предварительная обработка данных**:
		Убедитесь, что отсутствуют пропуски и выбросы, влияющие на графики.
		Логарифмируйте или стандартизируйте данные, если есть сильная асимметрия.
	**Используйте несколько методов одновременно**:
		Стройте гистограмму и KDE для каждого атрибута, чтобы сравнить фактическое и сглаженное распределения.
		Визуализируйте выбросы через boxplot.
	**Автоматизация анализа**:
		Используйте Python или R для создания графиков и расчета статистик автоматически для всех атрибутов.
	**Учитывайте тип данных**:
		Для категориальных данных используйте столбчатые диаграммы.
		Для числовых — методы выше.

**Пример анализа на практике**

Предположим, у нас есть датасет с атрибутами: возраст, доход, количество покупок. Мы хотим понять распределение каждого атрибута.

```python
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
from scipy.stats import shapiro 

# Пример датасета 
data = pd.DataFrame({ 
	'Возраст': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70], 
	'Доход': [30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000], 
	'Покупки': [5, 7, 8, 6, 10, 12, 15, 20, 22, 25] 
}) 
	
# Анализ для одного атрибута (например, "Возраст") 
column = 'Возраст' 

# Гистограмма и KDE 
sns.histplot(data[column], kde=True, bins=10) 
plt.title(f'Распределение: {column}') 
plt.show() 

# Boxplot 
sns.boxplot(x=data[column]) 
plt.title(f'Ящиковая диаграмма: {column}') 
plt.show() 

# Тест на нормальность 
stat, p = shapiro(data[column]) 

if p > 0.05: 
	print(f'{column}: Распределение нормально (p={p:.3f})') 
else: 
	print(f'{column}: Распределение НЕ нормально (p={p:.3f})') 
```

**Результаты:**
- **Графики** покажут форму распределения, наличие выбросов и асимметрию.
- **Статистика** уточнит, соответствует ли распределение нормальному или нет.

Аналогичные шаги можно повторить для остальных атрибутов.
## Для каждого атрибута присутствует графический и статистический анализы нормальности распределения данных

**Анализ нормальности распределения данных для каждого атрибута**

**1. Что включает анализ нормальности:**
- **Графический анализ:**
	Оценка визуальной формы распределения через гистограммы, плотность, Q-Q графики.
	Использование диаграмм размаха (boxplot) для выявления выбросов и асимметрии.

- **Статистический анализ:**
	Применение статистических тестов (например, Shapiro-Wilk, Anderson-Darling, D’Agostino’s K^2) для проверки гипотезы о нормальности.

- **Коэффициенты распределения:**
	Вычисление асимметрии (skewness) и эксцесса (kurtosis).

**2. Хорошие практики:**
- **Графический анализ:**
	Используйте комбинацию гистограмм, Q-Q графиков и диаграмм размаха для визуального анализа.
	Накладывайте теоретическую нормальную плотность (KDE) на гистограмму для сравнения.

- **Статистические тесты:**
	Применяйте несколько тестов нормальности для проверки гипотезы, т.к. разные тесты могут быть чувствительны к разным аспектам отклонений.

- **Обработка ненормальных данных:**
	Применяйте трансформации (логарифм, Box-Cox, Yeo-Johnson), если данные сильно отклоняются от нормальности.
	Учитывайте, что тесты нормальности могут быть избыточно чувствительными на больших выборках.

- **Документируйте результаты:**
	Записывайте выводы для каждого атрибута, включая значения тестов, графические интерпретации и предложенные действия.

**3. Пример на практике**
- **3.1. Импорт данных и библиотек**

```python
import pandas as pd 
import numpy as np 
import seaborn as sns 
import matplotlib.pyplot as plt 
from scipy.stats import shapiro, anderson, probplot, skew, kurtosis

# Генерация синтетических данных 
np.random.seed(42) data = pd.DataFrame({ 
	"Attribute_A": 
		np.random.normal(loc=50, scale=10, size=1000), 
	# Нормальное распределение 
	"Attribute_B": 
		np.random.exponential(scale=50, size=1000) 
	# Несимметричное распределение 
}) 
```

- **3.2. Графический анализ**
**Гистограммы с KDE**
```python
for column in data.columns: 
	sns.histplot(data[column], kde=True, bins=30, color='blue') 
	plt.title(f"Гистограмма и KDE для {column}") 
	plt.xlabel(column) plt.ylabel("Frequency") 
	plt.show() 
```

**Q-Q график**
```python
for column in data.columns: 
	probplot(data[column], dist="norm", plot=plt) 
	plt.title(f"Q-Q Plot для {column}") 
	plt.show() 
```

**Boxplot**
```python
for column in data.columns: 
	sns.boxplot(x=data[column], color='skyblue') 
	plt.title(f"Boxplot для {column}") 
	plt.xlabel(column) plt.show() 
```

- **3.3. Статистический анализ**
**Шапиро-Уилк тест**
```python
for column in data.columns: 
	stat, p_value = shapiro(data[column]) 
	print(f"{column} - Shapiro-Wilk Test: Statistic={stat:.3f}, p-value={p_value:.3f}") 

if p_value > 0.05: 
	print(f"{column}: Данные нормально распределены.\n") 
else: 
	print(f"{column}: Данные НЕ нормально распределены.\n") 
```

**Коэффициенты асимметрии и эксцесса**
```python
for column in data.columns: 
	skewness = skew(data[column]) 
	kurt = kurtosis(data[column]) 
	
	print(f"{column}:") 
	print(f" Асимметрия (Skewness): {skewness:.3f}") 
	print(f" Эксцесс (Kurtosis): {kurt:.3f}\n") 
```

- **3.4. Результаты и интерпретация**
	**Для Attribute_A (нормальное распределение):**
	- **Графики:** Гистограмма имеет форму колокола, а точки на Q-Q графике лежат близко к диагонали.
	- **Шапиро-Уилк тест:** p-value > 0.05 (гипотеза о нормальности принимается).
	- **Асимметрия и эксцесс:** Значения близки к 0, что подтверждает нормальность.

	**Для Attribute_B (ненормальное распределение):**
	- **Графики:** Гистограмма с длинным правым хвостом, а точки на Q-Q графике отклоняются от диагонали.
	- **Шапиро-Уилк тест:** p-value < 0.05 (гипотеза о нормальности отвергается).
	- **Асимметрия и эксцесс:** Асимметрия сильно положительная, эксцесс высокий, что подтверждает отклонение.

- **3.5. Обработка ненормальных данных (если нужно)**
Если данные сильно отклоняются от нормальности, можно применить трансформацию:

```python
# Применение логарифмической трансформации 
data['Attribute_B_Log'] = np.log1p(data['Attribute_B'])

# Повторный анализ после трансформации 
sns.histplot(data['Attribute_B_Log'], kde=True, bins=30, color='green') 
plt.title("Гистограмма и KDE для Attribute_B (Log-трансформация)") 
plt.xlabel("Attribute_B_Log") 
plt.ylabel("Frequency") 
plt.show()
```
## На основании полученных результатов сделан вывод о типе распределения

Чтобы сделать вывод о типе распределения данных на основе графических и статистических анализов, важно рассмотреть **графическую форму распределения**, **статистические метрики** и результаты **тестов на нормальность**.

**Основные типы распределений:**
- **Нормальное распределение (Гауссово)**:
	- Симметричное, "колоколообразное".
	- Среднее = медиана = мода.
	- Графически: гистограмма и KDE показывают симметрию, QQ-plot дает прямую линию.
	- Тесты на нормальность (Шапиро-Уилк, Колмогорова-Смирнова) дают p-value > 0.05.

- **Равномерное распределение**:
	- Все значения равновероятны.
	- Графически: плоская гистограмма.
	- Статистики показывают небольшой разброс, данные не имеют четкой центральной тенденции.

- **Экспоненциальное распределение**:
	- Распределение для времени ожидания между событиями.
	- Графически: асимметрия вправо, плавное убывание плотности.
	- Медиана меньше среднего.
	- Логарифм данных приводит их ближе к нормальному распределению.

- **Бимодальное распределение**:
	- Две отчетливые "вершины".
	- Графически: гистограмма с двумя пиками.
	- Среднее и медиана не дают точного представления из-за двух мод.

- **Скошенные распределения**:
	- **С положительной асимметрией (вправо)**: длинный хвост справа.
	- **С отрицательной асимметрией (влево)**: длинный хвост слева.
	- Графически: асимметричная гистограмма.
	- Коэффициенты асимметрии (скос > 0 для правой, < 0 для левой).

- **Логнормальное распределение**:
	- Данные с положительной асимметрией.
	- Логарифмирование приводит данные к нормальному распределению.
	- Часто встречается в доходах, финансах, биологии.

**Как отличить тип распределения**
- **1. Графический анализ**
	Постройте **гистограмму и KDE**: 
	- Симметрия → кандидат на нормальное распределение.
	- Пики → возможное бимодальное распределение.
	- Длинный хвост → асимметричное распределение.

	Используйте **QQ-Plot**: 
	- Прямая линия → нормальное распределение.
	- Отклонения от прямой → ненормальное распределение.

-  **2. Статистический анализ**
	**Меры центральной тенденции**: 
	- Нормальное распределение: среднее ≈ медиана ≈ мода.
	- Асимметричное: среднее сдвинуто в сторону хвоста.

	**Асимметрия и эксцесс**: 
	- Нормальное распределение: асимметрия ≈ 0, эксцесс ≈ 0.
	- Асимметрия > 0 или < 0 → скошенные распределения.
	- Эксцесс > 0 → острое распределение, < 0 → плоское.

- **3. Тесты на нормальность**
	**Шапиро-Уилк**, **Колмогорова-Смирнова**: 
	- p-value > 0.05 → нормальное распределение.
	- p-value < 0.05 → ненормальное.
	- Сравните результаты тестов с графиками.

**Алгоритм вывода о типе распределения**
- Постройте гистограмму и KDE для предварительного осмотра.
- Рассчитайте меры центральной тенденции, асимметрию и эксцесс.
- Проверьте тесты на нормальность.
- Постройте QQ-plot для подтверждения гипотезы.
- Сопоставьте все результаты: 
	- Симметричная гистограмма, p > 0.05 → нормальное распределение.
	- Асимметрия вправо/влево → скошенное распределение.
	- Два пика → бимодальное.
	- Прямая линия в лог-преобразовании → логнормальное распределение.

**Пример: Определение распределения для доходов**
```python
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
from scipy.stats import shapiro, kurtosis, skew, probplot 

# Данные 
data = pd.DataFrame({'Доход': [30000, 35000, 40000, 60000, 80000, 100000, 150000, 200000, 250000, 300000]}) 

# Графический анализ 
sns.histplot(data['Доход'], kde=True) 
plt.title('Гистограмма с KDE') 
plt.show() 

# QQ-Plot 
probplot(data['Доход'], dist="norm", plot=plt) 
plt.show() 

# Статистический анализ 
mean = data['Доход'].mean() 
median = data['Доход'].median() 
mode = data['Доход'].mode()[0] 
skewness = skew(data['Доход']) 
kurt = kurtosis(data['Доход']) 

print(f"Среднее: {mean}, Медиана: {median}, Мода: {mode}") 
print(f"Асимметрия: {skewness}, Эксцесс: {kurt}") 

# Тест на нормальность 
stat, p = shapiro(data['Доход']) 
print(f"p-value для теста Шапиро-Уилка: {p:.3f}") 

# Вывод 
if p > 0.05: 
	print("Распределение может быть нормальным.") 
else: 
	print("Распределение ненормальное.") 
```

**Оценка:**
- **График** покажет сильную правую асимметрию.
- **Среднее > медианы**, асимметрия > 0 → скошенное вправо.
- **Тест Шапиро-Уилка** даст p < 0.05 → распределение ненормальное.

**Вывод:** 
- Данные скорее всего логнормальны или имеют сильный правый хвост.

## Сформирован итоговый набор данных согласно требованиям

Формирование итогового набора данных (датасета) является ключевым шагом в любом проекте машинного обучения. Итоговый датасет должен соответствовать целям задачи, быть качественным и подходить для модели, которую вы планируете использовать. Приведу общий алгоритм и пример на практике.

**Алгоритм формирования датасета**
- **Определите цель задачи:**
	Выясните, какую проблему решает модель (классификация, регрессия, кластеризация и т.д.).
	Какие метрики будут использоваться для оценки модели.

- **Соберите данные:**
	Источники: базы данных, API, веб-скрапинг, готовые датасеты.
	Убедитесь, что данные релевантны задаче.

- **Очистите данные:**
	Удалите или обработайте пропуски (например, заполните медианой, удалите строки).
	Удалите выбросы или аномалии.
	Убедитесь, что данные не содержат дубликатов.

- **Инженерия признаков:**
	Создайте новые признаки из существующих данных.
	Проведите нормализацию или стандартизацию (для числовых признаков).
	Закодируйте категориальные признаки (One-Hot Encoding, Label Encoding).

- **Разделите данные:**
	На тренировочный, валидационный и тестовый наборы (обычно в пропорции 70%-15%-15%).

- **Проверка на сбалансированность:**
	Убедитесь, что целевая переменная не имеет дисбаланса (или используйте методы для борьбы с дисбалансом).

**Пример: Классификация кредитного риска**

**Задача:** Предсказать, выдавать ли кредит клиенту (да/нет).

**Шаги:**
- **Определение задачи:**
	Тип задачи: бинарная классификация.
	Метрика: F1-score (важно учитывать баланс между Precision и Recall).

**Сбор данных:**
	Данные клиентов из банка: возраст, доход, кредитная история, количество текущих кредитов и т.д.

**Очистка данных:**
- Пропущенные значения: 
	- Возраст: заполнить медианой.
	- Доход: пропущенные значения заменить на среднее.
- Выбросы: 
	- Исключить клиентов с доходом выше 99-го перцентиля, если такие аномалии нерелевантны.
	- Удалить дубликаты.

**Инженерия признаков:**
	Закодировать категориальные признаки, например, статус занятости (работает, безработный, пенсионер) через One-Hot Encoding.
	Нормализовать числовые признаки (возраст, доход) с помощью Min-Max Scaling.

**Разделение данных:**
	Тренировочные данные: 70%.
	Валидация: 15%.
	Тест: 15%.

**Проверка баланса целевой переменной:**
	Если в данных 90% отказов в кредите и 10% успешных заявок, используем технику Oversampling (например, SMOTE) для сбалансированности.

**Заключение**
	Итоговый набор данных должен быть хорошо очищен, содержать релевантные признаки и подходить для использования в модели. Для каждой задачи подход к формированию датасета может немного отличаться, но общие принципы остаются одинаковыми.


# Разведочный анализ данных
## Участник создал графический пользовательский интерфейс аналитической системы

Создание дашборда — это действительно отличное решение для реализации аналитической системы, если это соответствует требованиям задания. Дашборд позволяет визуализировать данные, сделать их доступными и удобными для анализа. Главное — правильно структурировать работу над проектом и учесть все требования к функциональности и визуализации.

**Почему дашборд — хорошая идея:**
- **Удобство:** Пользователи получают всю информацию в одном месте.
- **Визуализация:** Графики, таблицы и ключевые метрики помогают быстро понять основные тренды.
- **Интерактивность:** Фильтры, кнопки и другие элементы позволяют гибко взаимодействовать с данными.
- **Масштабируемость:** Дашборд можно легко доработать, добавив новые визуализации или данные.

**Как правильно сделать дашборд**
**1. Определите требования**
	**Цель дашборда:** Например, мониторинг продаж, отслеживание KPI или анализ клиентской базы.
	**Ключевая аудитория:** Кто будет использовать дашборд (аналитики, менеджеры, заказчики).
	**Функциональность:** 
	Какие данные должны быть отображены.
	Необходимые фильтры и параметры.
	Какие метрики и графики требуются.

**2. Выберите инструменты**
	**Инструменты для кодинга:** 
	**Dash (Python):** Подходит для простых и интерактивных дашбордов.

**3. Составьте структуру дашборда**
	Пример структуры:
	- **Шапка:** Название дашборда, период времени, глобальные фильтры.
	- **Панель ключевых метрик:** Общая выручка, средний чек, количество клиентов и т.д.
	- **Основная часть:** 
		Графики (линейные, круговые, столбчатые).
		Таблица с подробной информацией.
	- **Фильтры:** По категориям, регионам, временным периодам.

**4. Реализуйте дашборд**

Пример реализации дашборда с **Dash** (Python):

```python
import dash from dash 
import dcc, html 
import pandas as pd 
import plotly.express as px 


# Создание графиков 
line_fig = px.line(data, x="Дата", y="Продажи", title="Динамика продаж") 
pie_fig = px.pie(data, 
				 names="Категория", 
				 values="Продажи", 
				 title="Продажи по категориям"
				 ) 
				 
# Инициализация Dash-приложения 
app = dash.Dash(__name__) 

# Макет дашборда 
app.layout = html.Div([ 
	html.H1("Дашборд продаж"), 
	html.Div([ 
		html.Div([ 
			html.H3("Общая выручка"), 
			html.P(f"{data['Продажи'].sum()} USD") 
		], style={"display": "inline-block", "margin-right": "50px"}), 
		html.Div([ 
			html.H3("Средний чек"), 
			html.P(f"{data['Продажи'].mean():.2f} USD") 
		], style={"display": "inline-block"}) 
	]), 
	dcc.Graph(figure=line_fig), 
	dcc.Graph(figure=pie_fig), 
	dcc.Dropdown( 
		id="category-filter", 
		options=[{"label": cat, "value": cat} for cat in data["Категория"].unique()], 
		placeholder="Фильтр по категории" 
	), 
	html.Div(id="filtered-data") # Для отображения фильтрованных данных 
]) 

# Callback для обновления данных 
@app.callback( 
	dash.dependencies.Output("filtered-data", "children"),
	[dash.dependencies.Input("category-filter", "value")] 
) 

def update_table(selected_category): 
	if selected_category: 
		filtered_data = data[data["Категория"] == selected_category] 
	else: 
		filtered_data = data 
	return html.Table([ 
		html.Tr([html.Th(col) for col in filtered_data.columns]) + 
		[html.Tr([html.Td(filtered_data.iloc[i, j]) for j in range(filtered_data.shape[1])]) 
		for i in range(len(filtered_data))] ]) 

if __name__ == "__main__": 
	app.run_server(debug=True) 
```

**5. Тестирование**
	Проверьте, что дашборд соответствует требованиям задания.
	Убедитесь, что данные корректно обновляются при взаимодействии (например, при применении фильтров).
	Проверяйте производительность на больших объёмах данных.

**Хорошие практики при создании дашборда**
- **Минимализм:** Показывайте только ту информацию, которая действительно важна.
- **Цветовая палитра:** Используйте цвета для акцентов, избегая лишних визуальных шумов.
- **Интерактивность:** Реализуйте фильтры, раскрывающиеся графики и интерактивные таблицы.
- **Обновление данных:** Добавьте возможность обновлять данные автоматически или по запросу.
- **Документация:** Добавьте описание дашборда (например, подсказки при наведении).

**Вывод**
Создание дашборда, отвечающего требованиям задания, — это отличная идея. Используйте такие инструменты, как **Dash** или **Streamlit**, чтобы быстро реализовать его. Следуйте лучшим практикам проектирования, делая интерфейс понятным, интерактивным и полезным для конечного пользователя.

## Выполнено подключение к базе данных из аналитической системы

Подключение аналитической системы к базе данных позволяет динамически загружать и обрабатывать данные. В Python это можно сделать с использованием библиотек, которые поддерживают различные базы данных (PostgreSQL, MySQL, SQLite и другие). Вот пошаговая инструкция для подключения.

**1. Выбор библиотеки для подключения**

В зависимости от типа базы данных:

**PostgreSQL:** Используйте библиотеку psycopg2 или ORM-фреймворк SQLAlchemy.

**MySQL:** Используйте библиотеку mysql-connector-python или PyMySQL.

**SQLite:** Используйте стандартную библиотеку sqlite3.

**MS SQL Server:** Используйте библиотеку pyodbc или pymssql.

**2. Установка необходимых библиотек**

Установите библиотеки через pip. Например:

pip install psycopg2-binary # Для PostgreSQL pip install mysql-connector-python # Для MySQL pip install sqlalchemy # Для универсального ORM 

**3. Пример подключения**

```python
import psycopg2 

# Параметры подключения 
db_config = { 
			 "dbname": "your_database", 
			 "user": "your_username", 
			 "password": "your_password", 
			 "host": "localhost", 
			 "port": 5432 
		} 
		
# Подключение к базе данных 
try: 
	conn = psycopg2.connect(**db_config) 
	cursor = conn.cursor() cursor.execute("SELECT * FROM your_table LIMIT 10;") 
	rows = cursor.fetchall() 
	
	for row in rows: 
		print(row) 
	
	cursor.close() 
	conn.close() 
except Exception as e: 
	print(f"Ошибка подключения: {e}") 
```

**4. Интеграция подключения в аналитическую систему**
**Пример использования базы данных в дашборде (с использованием SQLAlchemy):**
SQLAlchemy — это мощный ORM-фреймворк, который упрощает работу с базами данных.

```python
from sqlalchemy 
import create_engine 
import pandas as pd 
from dash import Dash, dcc, html 

# Настройка подключения к базе данных 
db_url = "postgresql://your_username:your_password@localhost:5432/your_database" engine = create_engine(db_url) 

# Загрузка данных из базы в DataFrame 
query = "SELECT * FROM your_table LIMIT 10;" 
df = pd.read_sql(query, engine) 

# Создание Dash-приложения 
app = Dash(__name__) 
app.layout = html.Div([ 
	html.H1("Аналитический дашборд"), 
	html.Table([ html.Tr([html.Th(col) for col in df.columns]) + 
	[html.Tr([html.Td(df.iloc[i][col]) for col in df.columns]) for i in range(len(df))] ]) ]) 
	
if __name__ == "__main__": 
	app.run_server(debug=True)
```

**5. Практические советы**

**Безопасность:**
Никогда не храните пароли или конфиденциальную информацию в коде. Используйте переменные окружения или менеджеры секретов.
Пример с использованием os.environ: import os from sqlalchemy import create_engine db_url = os.getenv("DATABASE_URL") # Экспортируйте URL базы в переменную окружения engine = create_engine(db_url) 

**Оптимизация запросов:**
Используйте индексы в таблицах базы данных для ускорения выборок.
Загружайте только нужные данные (ограничивайте выборку).

**Пул соединений:**
Для высоконагруженных систем настройте пул соединений (поддерживается в SQLAlchemy).

**Логирование:**
Включите логирование запросов, чтобы отлаживать ошибки и анализировать производительность.

## Дашборд удовлетворяет основным требованиям построения (интерактивность, наличие фильтров, анализ актуальных показателей)

Для создания интерактивного и функционального дашборда с использованием Python и Dash необходимо пройти несколько этапов настройки и проверки. Вот основные шаги с объяснениями и примером кода.

**1. Структура Дашборда**
	Дашборд должен включать:
		**Интерактивные элементы**: Выпадающие списки, переключатели, слайдеры.
		**Фильтры**: Возможность фильтровать данные по ключевым параметрам.
		**Графики и таблицы**: Для отображения актуальных показателей.

**3. Пример Дашборда**
	Вот пример интерактивного дашборда, анализирующего данные о продажах.

**Код:**

```python
import dash from dash 
import dcc, html, Input, Output 
import pandas as pd 
import plotly.express as px 

# Загрузка данных 
data = pd.DataFrame({ 
	"Дата": pd.date_range(start="2023-01-01", periods=100), 
	"Продукт": ["Продукт А", "Продукт B"] * 50, 
	"Продажи": [x * 10 for x in range(100)], 
	"Регион": ["Север", "Юг"] * 50 
}) 

# Создание приложения 
app = dash.Dash(__name__) 

# Макет приложения 
app.layout = html.Div([ 
	html.H1("Дашборд продаж", style={"textAlign": "center"}), 
	html.Div([ 
			dcc.Dropdown( id="product_filter", options=[{"label": prod, "value": prod} for prod in data["Продукт"].unique()], 
						value=data["Продукт"].unique()[0], 
						clearable=False, 
						style={"width": "30%"} 
			), 
			dcc.Dropdown( id="region_filter", options=[{"label": reg, "value": reg} for reg in data["Регион"].unique()], 
						value=data["Регион"].unique()[0], 
						clearable=False, 
						style={"width": "30%"} 
			) 
	], style={"display": "flex", "justifyContent": "space-around"}), 
		dcc.Graph(id="sales_graph"), 
		html.Div(id="summary_text", 
					style={"textAlign": "center", "marginTop": "20px"}) 
	]) 
	
# Логика взаимодействия 
@app.callback( 
	[Output("sales_graph", "figure"), 
	Output("summary_text", "children")], 
	[Input("product_filter", "value"), 
	Input("region_filter", "value")]
) 

def update_dashboard(selected_product, selected_region): 
	# Фильтрация данных 
	filtered_data = data[ (data["Продукт"] == selected_product) & (data["Регион"] == selected_region) ] 
	
	# Построение графика 
	fig = px.line( filtered_data, x="Дата", y="Продажи", title=f"Продажи {selected_product} в регионе {selected_region}", markers=True ) 
	
	# Текстовая сводка 
	summary = f"Всего продаж: {filtered_data['Продажи'].sum()}" return fig, summary 
	

# Запуск приложения 
if __name__ == "__main__": 
	app.run_server(debug=True) 
```

**4. Настройка**
- **Интерактивность**:
	Используйте dcc.Dropdown, dcc.Slider, dcc.Input для получения данных от пользователя.
	Используйте декораторы @app.callback для связи элементов интерфейса с функциями.

- **Фильтры**:
	Включите ключевые фильтры (например, по датам, категориям, регионам).
	Убедитесь, что фильтры динамически обновляют отображаемые данные.

- **Анализ актуальных показателей**:
	Рассчитывайте сводные данные (например, общие продажи, средние значения).
	Отображайте их в текстовой форме или графическом виде.

**5. Проверка**
- **Тестирование:**
	**Функциональность фильтров**:
		Проверьте, что изменение значений в фильтрах корректно обновляет графики и сводки.
	**Обработка ошибок**:
		Убедитесь, что приложение корректно работает с пустыми фильтрами или отсутствием данных.
	**Производительность**:
		Тестируйте дашборд на больших объемах данных.
		Если требуется, используйте dcc.Store для хранения промежуточных данных.
	**Визуализация**:
		Проверьте читаемость графиков (подписи, масштаб, цвета).
		Убедитесь, что интерфейс адаптируется под разные размеры экранов.
		Этот подход поможет вам настроить профессиональный дашборд, соответствующий современным требованиям аналитики и визуализации данных.
## Визуализация отражает динамику полученных метрик во времени

Как аналитики метрополитена, мы будем создавать дашборд, который поможет эффективно отслеживать и анализировать ключевые показатели работы транспортной системы. Наш дашборд должен отражать динамику метрик во времени, чтобы оперативно выявлять тренды, принимать решения и оптимизировать работу метрополитена.

**1. Определите цели и пользователей**
- **Цели:** 
	Отслеживание пассажиропотока (по станциям, линиям и времени).
	Мониторинг пунктуальности поездов.
	Анализ загрузки инфраструктуры (перегрузки в час пик, простой вагонов и станций).
	Оптимизация графиков движения.
	Снижение расходов на обслуживание.

- **Пользователи:** 
	Руководство (для стратегических решений).
	Оперативные диспетчеры (для мониторинга и быстрого реагирования).
	Служба эксплуатации (для анализа технического состояния и планирования ремонта).

**2. Выберите ключевые метрики**
- Для метрополитена это могут быть:
	**Пассажиропоток:** общее количество пассажиров за день, час, месяц.
	**Загрузка станций:** пиковая и средняя нагрузка.
	**Интервалы движения поездов:** соответствие графику, отклонения.
	**Технические параметры:** количество простоев, аварий, неисправностей.
	**Доходы и расходы:** выручка от билетов, затраты на обслуживание.

**3. Определите временные диапазоны**
- Наш дашборд должен позволять анализировать данные в следующих разрезах:
	Время суток (час пик vs. межпиковое время).
	День недели (рабочие дни vs. выходные).
	Сезонные колебания (лето vs. зима).
	Годовые тенденции (YoY).

**4. Выберите визуализации**
- Используем подходящие графики для каждой задачи:
	**Линейные графики:** для отслеживания пассажиропотока по времени.
	**Тепловые карты:** для отображения загруженности станций в разные часы и дни.
	**Гистограммы:** для сравнения доходов или нагрузки по станциям или линиям.
	**Диаграммы Ганта:** для анализа расписания поездов и их отклонений.

**5. Добавьте фильтры и интерактивность**
- Наш дашборд должен позволять пользователям:
	Фильтровать данные по линиям и станциям.
	Выбирать временной интервал (час, день, неделя, месяц).
	Сравнивать показатели разных периодов.
	Переключаться между метриками (например, пассажиропоток и доходы).

**6. Обеспечьте корректность данных**
	**Источники данных:** данные с турникетов, сенсоров поездов, систем видеонаблюдения.
	**Обновление:** автоматическое обновление данных в реальном времени.
	**Проверка:** регулярная проверка на точность и консистентность данных.

**7. Соберите обратную связь**
	Протестируйте дашборд на диспетчерах и руководстве.
	Убедитесь, что информация читается легко и помогает принимать решения.

**8. Оптимизируйте дизайн**
	**Фокус на важных метриках:** пассажиропоток, отклонения от расписания.
	**Визуальная иерархия:** ключевые показатели — сверху, второстепенные ниже.
	**Использование цветовых кодов:** перегрузка (красный), нормальная нагрузка (зеленый).

**9. Регулярно обновляйте и улучшайте дашборд**
	Адаптируйте дашборд под новые задачи (например, изменения маршрутов или запуск новых станций).
	Добавляйте данные о внешних факторах (погода, городские мероприятия).
	Следуя этому подходу, мы создадим дашборд, который будет эффективно показывать динамику ключевых метрик метрополитена, помогая улучшать качество обслуживания и управление инфраструктурой.

## Рассмотрено не менее трёх методов кластеризации

**Три метода кластеризации:**

**K-Means (k-средние)** 
Разделяет данные на kk кластеров, минимизируя внутрикластерное расстояние. Центры кластеров (средние значения) обновляются до тех пор, пока алгоритм не сойдется.

- **Плюсы:** 
	Простота реализации.
	Эффективен для больших наборов данных.
	Хорошо работает для сферических кластеров одинакового размера.

- **Минусы:** 
	Требует задания количества кластеров (kk).
	Чувствителен к выбросам.
	Не подходит для кластеров сложной формы.

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** 
Обнаруживает кластеры на основе плотности точек. Точки, находящиеся в плотных регионах, объединяются в кластеры, а редкие точки остаются как шум.

- **Плюсы:** 
	Не требует предварительного задания числа кластеров.
	Хорошо работает с кластерами произвольной формы.
	Устойчив к выбросам.

- **Минусы:** 
	Параметры (ε\varepsilon и minPtsminPts) сильно влияют на результат.
	Может не работать при варьирующейся плотности кластеров.

**Иерархическая кластеризация (Hierarchical Clustering)** 
Построение дерева кластеров (дендрограммы), которое показывает, как данные объединяются на разных уровнях.

- **Плюсы:** 
	Не требует задания числа кластеров.
	Позволяет исследовать данные на разных уровнях детализации.

- **Минусы:** 
	Высокая вычислительная сложность для больших наборов данных.
	Чувствителен к шуму и выбросам.

**Как выбрать метод?**
**Понять природу данных** 
- **Тип данных:** 
	K-Means лучше подходит для численных данных с равномерными распределениями.
	DBSCAN эффективен для пространственных данных с плотностью.
	Иерархическая кластеризация полезна для малых наборов данных с неоднородными структурами.

- **Форма кластеров:** 
	Если ожидаются сферические (равномерные) кластеры, попробуйте K-Means.
	Если кластеры произвольной формы, начните с DBSCAN.

- **Размер данных:** 
	K-Means и DBSCAN подходят для больших данных.
	Иерархическая кластеризация предпочтительна для небольших наборов.

- **Исследуйте характеристики данных** 
	Проверьте наличие шума и выбросов. Если шум значителен, DBSCAN может быть лучшим выбором.
	Оцените плотность кластеров. Если плотность сильно варьируется, DBSCAN может работать хуже.

**Как выбрать лучший метод из трёх?**
- **Проведите эксперимент:**
	Примените все три метода к данным.
	Настройте параметры каждого метода (например, kk для K-Means, ε\varepsilon для DBSCAN).

- **Сравните результаты:**
	Используйте метрики, такие как силуэтный коэффициент или ARI, для сравнения качества кластеров.
	Визуализируйте кластеры, чтобы оценить их структуру.

- **Оцените устойчивость:**
	Проверьте, насколько результаты стабильны при изменении параметров или случайной инициализации.
	Таким образом, вы сможете выбрать метод, который лучше всего отвечает требованиям задачи.

## Определены показатели оценки качества кластеризации

Оценка качества кластеризации — это ключевая часть анализа, позволяющая понять, насколько хорошо данные разделены на кластеры. Лучшие показатели зависят от типа данных, метода кластеризации и целей анализа.

**Основные показатели оценки качества кластеризации:**
**1. Силуэтный коэффициент (Silhouette Coefficient)**
Измеряет, насколько точки похожи на точки внутри своего кластера по сравнению с точками из других кластеров.
**Диапазон:** от -1 до 1 (чем ближе к 1, тем лучше).

**Использование:** 
Подходит для любого метода кластеризации.
Хорош для оценки компактности и разделимости кластеров.

**Минусы:** 
Может быть некорректным при кластерах сложной формы.

**2. Индекс Дэвиса-Болдина (Davis-Bouldin Index)**
Оценивает отношение внутрикластерной компактности к межкластерной разделимости.
**Диапазон:** от 0 (чем меньше, тем лучше).

**Использование:** 
Работает для числовых данных.
Учитывает расстояние между центроидами кластеров.

**Минусы:** 
Чувствителен к шуму и выбросам.

**3. V-Measure (метрика энтропии)**
Оценивает качество кластеров с точки зрения однородности (точки одного класса в одном кластере) и полноты (все точки одного класса распределены по одному кластеру).
**Диапазон:** от 0 до 1 (чем ближе к 1, тем лучше).

**Использование:** 
Полезна, если известны истинные метки данных.

**4. Adjusted Rand Index (ARI)**
Измеряет схожесть между кластеризацией и истинными метками данных, с учетом случайного совпадения.
**Диапазон:** от -1 до 1 (чем ближе к 1, тем лучше).

**Использование:** 
Применяется для сравнения с известной разметкой данных.

**5. Коэффициент Компактности и Разделимости (Compactness and Separation)**
**Компактность:** измеряет, насколько близки точки внутри кластера.
**Разделимость:** измеряет, насколько кластеры отделены друг от друга.

**Использование:** 
Помогает оценить баланс между плотностью внутри кластера и расстоянием между кластерами.

**6. Сравнение внутрикластерного расстояния и межкластерного расстояния**
Вычисляет средние расстояния между точками внутри одного кластера и между центроидами разных кластеров.
**Диапазон:** Чем больше разница, тем лучше.


**Как выбрать лучший показатель?**
- **Исследуйте характер данных и метода кластеризации:**
	Для плотностных методов (например, DBSCAN) силуэтный коэффициент может быть лучше.
	Для K-Means подойдут силуэтный коэффициент и индекс Дэвиса-Болдина.
	Для кластеров с истинными метками лучше использовать ARI или V-Measure.

- **Используйте несколько метрик:**
	Примените несколько показателей одновременно. Например, силуэтный коэффициент и индекс Дэвиса-Болдина помогут получить более комплексную картину.

- **Сравните результаты визуально:**
	Если данные можно визуализировать (2D или 3D), проверьте, соответствуют ли кластеры логике задачи.

- **Учитывайте особенности задачи:**
	Если важна интерпретируемость, используйте метрики, которые легко объяснить коллегам или руководству (например, силуэтный коэффициент).
	Если данные сильно зашумлены, лучше избегать метрик, чувствительных к шуму (например, индекс Дэвиса-Болдина).

- **Оцените устойчивость метрики:**
	Проверьте, насколько метрика стабильна при изменении параметров модели (например, kk в K-Means или ε\varepsilon в DBSCAN).

**Пример выбора:**
	Если у вас есть истинные метки, используйте **ARI** или **V-Measure**.
	Если данных много, и форма кластеров неизвестна, начните с **силуэтного коэффициента**.
	Для анализа разделимости кластеров используйте **индекс Дэвиса-Болдина**.
	Лучший показатель — это тот, который наилучшим образом оценивает ключевые аспекты кластеризации, важные для вашей задачи.

## Выявлено от 3 до 5 кластеров

Для разделения данных на определенное количество кластеров (например, от 3 до 5) с использованием методов **K-Means**, **DBSCAN** и **Иерархической кластеризации**, необходимо учитывать их особенности и подходы к заданию кластеров. Вот подробное объяснение для каждого метода:

**1. K-Means (k-средние)**
**Шаги:**
**Задайте количество кластеров kk:**
	Укажите число кластеров, например, k=3k = 3, k=4k = 4, k=5k = 5.
	Если число кластеров неизвестно, используйте метод "локтя" или силуэтный коэффициент для выбора оптимального kk.

**Инициализация центроидов:**
	Алгоритм случайно выбирает kk начальных центров кластеров.

**Присвоение точек кластерам:**
	Каждая точка присваивается ближайшему центроиду, используя метрику расстояния (например, Евклидово расстояние).

**Обновление центроидов:**
	Центры кластеров пересчитываются как среднее значение точек в каждом кластере.

**Итерации:**
	Шаги 3 и 4 повторяются, пока центроиды не перестанут изменяться или не будет достигнуто максимальное число итераций.

**Особенности:**
	K-Means **обязательно разделяет данные на заданное число кластеров**.
	Подходит для сферических кластеров и равномерно распределенных данных.



**2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**
**Шаги:**
**Определите параметры ε\varepsilon (радиус поиска) и minPtsminPts (минимальное число точек в кластере):**
	ε\varepsilon: радиус окрестности точки, в пределах которого точки считаются соседями.
	minPtsminPts: минимальное количество точек для образования плотного кластера.

**Обнаружение кластеров:**
	Алгоритм выбирает произвольную точку и проверяет, есть ли вокруг неё минимум minPtsminPts соседей в радиусе ε\varepsilon.
	Если да, точка становится ядром кластера, и соседи включаются в кластер.
	Если нет, точка помечается как шум.

**Рост кластеров:**
	Алгоритм повторяет процесс для соседей точки, пока не будут покрыты все плотные области.

**Кластеры формируются:**
	После завершения процесса кластеры и точки шума определены.

**Особенности:**
	DBSCAN **не требует явного задания количества кластеров**, но позволяет выделить кластеры сложной формы.
	Чтобы получить от 3 до 5 кластеров, нужно экспериментировать с параметрами ε\varepsilon и minPtsminPts.
	Подходит для задач с шумными и неравномерно распределенными данными.

**Как настроить ε\varepsilon и minPtsminPts:**
	Используйте графики расстояний (k-distance plot), чтобы найти оптимальный ε\varepsilon.
	Подберите minPtsminPts, ориентируясь на минимальную плотность кластеров.

**3. Иерархическая кластеризация**
**Шаги:**
**Выберите тип связи (linkage):**
	**Single linkage:** минимальное расстояние между точками.
	**Complete linkage:** максимальное расстояние между точками.
	**Average linkage:** среднее расстояние между точками.
	**Ward linkage:** минимизация внутрикластерной дисперсии (подходит для большинства задач).

**Постройте дендрограмму:**
	Алгоритм инициализирует каждую точку как отдельный кластер.
	Последовательно объединяет ближайшие кластеры на основе выбранного типа связи.
	Построение продолжается до тех пор, пока все данные не окажутся в одном большом кластере.

**Выделите кластеры:**
	Для выделения от 3 до 5 кластеров обрежьте дендрограмму на нужном уровне (горизонтальная линия).
	Это разделит данные на указанное количество кластеров.

**Особенности:**
	Не требует задания числа кластеров на старте, но позволяет гибко выбрать их количество по дендрограмме.
	Подходит для небольших наборов данных, так как требует больше ресурсов для вычислений.

**Как выбрать подходящий метод?**
**Если количество кластеров известно заранее:**
	Используйте **K-Means**, так как он точно разделит данные на заданное число кластеров.
**Если форма кластеров сложная или данные содержат шум:**
	Используйте **DBSCAN**, так как он может выявлять кластеры произвольной формы.
	Настройте параметры ε\varepsilon и minPtsminPts, чтобы получить нужное количество кластеров.
**Если данные малые и структура кластеров неизвестна:**
	Используйте **Иерархическую кластеризацию**, чтобы визуально определить оптимальное число кластеров по дендрограмме.

Выбор метода зависит от ваших данных и задачи. Начните с анализа данных (распределения, формы кластеров, наличия шума) и протестируйте каждый метод, чтобы определить лучший.

## Кластеры определены и поименнованы

Определение и именование кластеров после выполнения кластеризации с использованием методов K-Means, DBSCAN и Иерархической кластеризации — это важный шаг, позволяющий понять, что представляют собой кластеры. Этот процесс включает анализ характеристик точек, принадлежащих каждому кластеру, и присвоение кластерам осмысленных имен.

**Шаги для определения и именования кластеров**
**1. Изучите характеристики точек внутри кластеров**
	Для каждого кластера вычислите следующие статистики: 
		Средние значения (или медианы) признаков.
		Диапазон значений признаков.
		Распределение категориальных признаков (если таковые имеются).
		Используйте эти данные, чтобы понять, чем выделяется каждый кластер.

**2. Сравните кластеры друг с другом**
	Определите, какие признаки наиболее сильно различают кластеры.
	Найдите уникальные свойства или особенности, характерные для каждого кластера.

**3. Присвойте имена на основе смысла данных**
	Имена должны отражать ключевые свойства точек, принадлежащих кластеру.
	Например, в задаче сегментации клиентов можно назвать кластеры: "Лояльные клиенты", "Редкие покупатели", "Новички".

**Особенности именования кластеров для разных методов**
**1. K-Means**
	К-Means создает компактные и разделенные кластеры.
**Процесс:** 
	Анализируйте центроиды кластеров (средние значения признаков в каждом кластере).
	Сравните центроиды кластеров, чтобы определить ключевые различия.
	Используйте визуализацию (например, scatter plot, pair plot), чтобы понять, как кластеры распределены в пространстве признаков.
**Пример:** 
	Если анализируется пассажиропоток метрополитена, кластеры могут быть названы "Час пик", "Межпиковое время", "Выходные дни" на основе средней плотности пассажиров.

**2. DBSCAN**
	DBSCAN выявляет плотные области данных, оставляя разреженные точки как "шум".
**Процесс:** 
	Для каждого кластера определите средние или медианные значения ключевых признаков.
	Проанализируйте географическую или пространственную структуру, если данные связаны с координатами.
	Выделите шум и подумайте, как его интерпретировать (например, выбросы или аномалии).
**Пример:** 
	Если это географические данные, кластеры можно назвать "Центральные станции", "Периферия", а шум — "Случайные выбросы".

**3. Иерархическая кластеризация**
	Иерархическая кластеризация позволяет выделять кластеры на разных уровнях детализации.
**Процесс:** 
	Постройте дендрограмму и выберите уровень, на котором кластеры делятся на 3–5 групп.
	Изучите статистики для каждой группы точек.
	Сравните кластеры между уровнями дендрограммы, чтобы понять их внутреннюю структуру.
**Пример:** 
	Если данные о продажах, кластеры могут быть названы "Высокий доход", "Средний доход", "Низкий доход".

**Общие инструменты для анализа и именования кластеров**
**Визуализация:**
	Постройте графики распределения признаков (гистограммы, boxplot) для каждого кластера.
	Используйте PCA или t-SNE для снижения размерности и визуализации кластеров в 2D/3D.
**Статистический анализ:**
	Используйте тесты (например, ANOVA или t-тест) для проверки значимых различий между кластерами.
**Включение доменной экспертизы:**
	Если вы работаете в определенной области (например, транспорт, ритейл, медицина), проконсультируйтесь с экспертами для осмысленного именования кластеров.
**Автоматизация анализа признаков:**
	Примените алгоритмы для определения наиболее значимых признаков (например, feature importance из моделей машинного обучения).


**Пример:**
Допустим, вы кластеризовали пассажиров метрополитена:


**K-Means:** 
	Центроид 1: Высокий поток пассажиров утром. Название: "Час пик (утро)".
	Центроид 2: Умеренный поток в обед. Название: "Межпиковое время".
	Центроид 3: Низкий поток ночью. Название: "Ночная смена".

**DBSCAN:**
	Кластер 1: Плотные точки в центре города. Название: "Центральные станции".
	Кластер 2: Редкие точки на окраине. Название: "Периферия".
	Шум: Точки без явной структуры. Название: "Шумовые данные".

**Иерархическая кластеризация:** 
	Кластер 1: Высокий доход. Название: "Активные пользователи".
	Кластер 2: Средний доход. Название: "Среднестатистические пользователи".

Итог: Анализируйте признаки, используйте визуализации и учитывайте контекст задачи для осмысленного именования кластеров.


## В обосновании присутствует визуализация распределения данных по кластерам

**Как включить визуализацию в обоснование кластеризации**

Визуализация — это мощный инструмент для объяснения результатов кластеризации. Она помогает показать распределение данных по кластерам, а также выявить связи и различия между ними. Однако, если атрибутов много или данные сложные, визуализация требует специальных подходов.

**Проблема 1: Многомерные данные**
**Решение: Снижение размерности**
Когда данные имеют множество признаков, графики становятся многомерными, что сложно интерпретировать. Для визуализации:

**Методы снижения размерности:**
**PCA (Principal Component Analysis):** 
Снижает размерность до 2D или 3D, сохраняя как можно больше вариации в данных.
Помогает понять глобальные различия между кластерами.

**t-SNE (t-Distributed Stochastic Neighbor Embedding):** 
Сохраняет локальные структуры данных.
Подходит для визуализации сложных данных, но не всегда сохраняет глобальную структуру.

**UMAP (Uniform Manifold Approximation and Projection):** 
Быстрее t-SNE, хорошо показывает как локальные, так и глобальные структуры.

**Выбор важных признаков:**
Используйте методы, такие как Feature Importance из ансамблевых моделей (например, Random Forest), чтобы выбрать ключевые признаки для построения графиков.
Анализируйте распределения кластеров только по наиболее значимым признакам.

**Пары признаков:**
Создайте диаграммы рассеяния (scatter plot) для всех пар признаков (например, с помощью Pair Plot из Seaborn).
Ищите, какие признаки лучше всего разделяют кластеры.

**Проблема 2: Сложности в интерпретации кластеров**
Если визуализация не позволяет однозначно понять, что представляют кластеры, можно использовать следующие подходы:

**1. Анализ характеристик точек в кластере**
Рассчитайте для каждого кластера: 
Средние, медианные и стандартные отклонения всех признаков.
Доли категорий для категориальных признаков.
Сравните кластеры, чтобы найти уникальные особенности.

Например: 
Кластер 1: высокая плотность пассажиров в утренние часы.
Кластер 2: низкая плотность ночью.

**2. Центроид как представление кластера**
Для **K-Means** и **Иерархической кластеризации**: 
Используйте центроиды кластеров как репрезентативные значения.

Для **DBSCAN**: 
Рассчитайте медианные значения признаков для точек в каждом кластере.

**3. Таблицы с описанием кластеров**
Создайте таблицу, где для каждого кластера указаны: 
Средние значения ключевых признаков.
Основные отличия от других кластеров.
Это поможет лучше понять природу кластеров.

**Проблема 3: Большой объем данных**
Когда в данных тысячи точек, графики становятся перегруженными, и разглядеть структуру сложно.

**Решения:**
**Уменьшение объема данных для визуализации:**
**Сэмплирование:** 
Возьмите случайную подвыборку данных (например, 10-20% от общего объема).
Это сохраняет общую структуру данных, но уменьшает плотность точек на графике.

**Уменьшение плотности:** 
Используйте методики агрегации (например, кластеризацию внутри кластера для сглаживания распределения).

**Тепловые карты:**
Постройте тепловую карту, показывающую плотность точек в разных областях пространства признаков.

**3D-визуализация:**
Если данных много, используйте интерактивные 3D-графики (например, с Plotly), чтобы исследовать данные в динамике.

**Визуализация центроидов:**
Вместо точек изобразите только центроиды или их окружения (например, области вокруг центроидов).

**Кластеризация кластеров:**
Если количество точек слишком велико, проведите еще одну кластеризацию на уровне уже найденных кластеров, чтобы упростить структуру.

**Итоговый алгоритм:**
**Анализ и выбор признаков:**
Выберите значимые признаки с помощью анализа или снижения размерности.

**Построение визуализаций:**
Используйте графики рассеяния, 2D/3D-проекции и тепловые карты.

**Описание кластеров:**
Рассчитайте статистики для каждого кластера.
Представьте их в виде таблиц или графиков.

**Документирование результатов:**
Опишите особенности каждого кластера с опорой на визуализации и статистику.
Используйте понятные названия для кластеров.
Эти шаги помогут интерпретировать кластеры даже при сложных и многомерных данных.

## Выполнен визуальный анализ оценки качества кластеризации

Визуальный анализ оценки качества кластеризации позволяет понять, насколько хорошо кластеры отражают структуру данных. Такой анализ используется для проверки разбиения данных, выявления ошибок и выбора наиболее подходящего метода кластеризации. Ниже описаны ключевые шаги и методы для выполнения визуальной оценки.

**1. Построение графиков для визуализации кластеров**
**a. Scatter Plot (Диаграмма рассеяния)**
Если данных немного (2–3 признака), можно отобразить их на плоскости или в 3D-пространстве.
Точки раскрашиваются в зависимости от принадлежности к кластеру.
Используйте цвета, чтобы выделить кластеры, и проверяйте, перекрываются ли области.
**Подходит для K-Means, DBSCAN, иерархической кластеризации.**

**b. PCA, t-SNE или UMAP**
Для многомерных данных сначала снизьте размерность до 2D или 3D.
Примените **PCA**, **t-SNE** или **UMAP** для визуализации.
Отображение кластеров в низкоразмерном пространстве помогает понять их распределение.

**c. Гистограммы и Boxplots**
Постройте гистограммы или boxplots (коробчатые диаграммы) для каждого признака по кластерам.
Проверьте, насколько различаются распределения признаков между кластерами.

**2. Визуализация плотности и структуры кластеров**
**a. Heatmap (Тепловая карта)**
Постройте тепловую карту, показывающую плотность точек в каждом кластере.
Используйте это, чтобы понять, насколько равномерно распределены точки внутри кластеров.

**b. Silhouette Plot (График силуэтного коэффициента)**
Постройте силуэтный график для оценки качества кластеризации: 
Силуэтный коэффициент измеряет, насколько точки в кластере похожи друг на друга и отличаются от соседних кластеров.
Коэффициент должен быть > 0.5 для большинства точек.
График показывает распределение значений силуэта для каждого кластера.

**3. Проверка границ кластеров**
**a. Decision Boundaries (Границы решений)**
Постройте границы кластеров в 2D-пространстве: 
Выберите 2 признака (или используйте PCA), чтобы построить области, принадлежащие каждому кластеру.
Это полезно для визуализации K-Means и DBSCAN.

**b. Визуализация ядер плотности (Density Plots)**
Для методов вроде DBSCAN постройте графики плотности данных, чтобы увидеть, как кластеры соответствуют плотным областям.

**4. Оценка перекрытия кластеров**
Используйте визуализацию для анализа перекрывающихся областей: 
Перекрытие может быть видно на scatter plot или PCA-графиках.
Если области кластеров сильно пересекаются, метод кластеризации, вероятно, работает недостаточно хорошо.

**5. Обнаружение выбросов**
Выбросы можно визуализировать с помощью DBSCAN: точки, помеченные как шум, выделяются отдельно.
Постройте график распределения точек с метками "шум/кластер".

**6. Анализ по распределению признаков**
Постройте матрицу scatter plots (например, Seaborn Pair Plot), чтобы увидеть, как точки распределены по парам признаков в кластерах.
Это помогает выявить ключевые различия между кластерами.

**Пример последовательности визуализации для K-Means**
Снизьте размерность данных с помощью PCA до 2D.
Постройте scatter plot с цветовой дифференциацией кластеров.
Постройте силуэтный график для оценки качества кластеризации.
Постройте boxplot для анализа распределения ключевых признаков по кластерам.
Если границы кластеров не очевидны, измените параметры kk и повторите визуализацию.

**Итог: Комбинирование визуализаций**
Для качественного анализа используйте несколько подходов:
Визуализация кластеров в пространстве (scatter plot, PCA, t-SNE).
Оценка плотности (тепловые карты, ядерные графики плотности).
Оценка разделения (силуэтный график, boxplot).
Эти методы помогут лучше понять, насколько хорошо ваши данные кластеризованы, и выявить возможные проблемы (перекрытие, шум или неправильное количество кластеров).


## Определен лучший алгоритм кластеризации с учётом выбранной метрики

Подумай сам бро.

# Построение, обучение и оптимизация модели
## Исходные данные проверены на сбалансированность

**1. Как проверить данные на сбалансированность?**
**Проверка дисбаланса:**
**Распределение классов:** Построить гистограмму или круговую диаграмму, чтобы увидеть количество объектов в каждом классе.
**Числовая оценка:** Рассчитать процентное соотношение классов.

```python
import pandas as pd 
import matplotlib.pyplot as plt 

# Пример данных
data = {'Class': ['A', 'A', 'B', 'A', 'B', 'B', 'A', 'C', 'C', 'C', 'C']} 
df = pd.DataFrame(data) 

# Распределение классов 
class_counts = df['Class'].value_counts() print(class_counts) 

# Визуализация 
class_counts.plot(kind='bar') 
plt.title('Распределение классов') 
plt.xlabel('Классы') 
plt.ylabel('Количество') 
plt.show() 
```
## Дисбаланс принадлежности к классам отсутствует или устранен

**2. Как устранить дисбаланс классов, если он присутствует?**

**Методы:**
**Oversampling (увеличение меньшинства):**
Использовать алгоритмы вроде SMOTE (Synthetic Minority Oversampling Technique).
from imblearn.over_sampling import SMOTE smote = SMOTE(random_state=42) X_resampled, y_resampled = smote.fit_resample(X, y) 

**Undersampling (уменьшение большинства):**
Уменьшить количество объектов в доминирующем классе.
from imblearn.under_sampling import RandomUnderSampler undersampler = RandomUnderSampler(random_state=42) X_resampled, y_resampled = undersampler.fit_resample(X, y) 

**Кост-функции, учитывающие дисбаланс:**
Например, при использовании LogisticRegression добавьте параметр class_weight='balanced'.

**Генерация дополнительных данных:**
Создать больше данных для недопредставленного класса (аугментация).


## Рассмотрено не менее трёх моделей классификации

**3. Примеры 3 моделей классификации**
**Модели:**
**Logistic Regression (Логистическая регрессия):** Простая линейная модель.
**Random Forest (Случайный лес):** Ансамблевая модель деревьев решений.
**Gradient Boosting (XGBoost):** Продвинутая модель бустинга.

## Выбрана модель классификации

**4. Как выбрать лучшую модель для конкретной задачи?**
**Алгоритм выбора:**
**Исследование данных:**
Если классы линейно разделимы, попробуйте Logistic Regression.
Если данные сложные и нелинейные, используйте Random Forest или XGBoost.

**Скорость обучения:**
Logistic Regression быстрее обучается, подходит для больших данных.
Random Forest и XGBoost требуют больше ресурсов.

**Гипотезы:**
Если данные содержат важные взаимодействия признаков, XGBoost может быть более эффективным.


## Выбранная модель обучена

**5. Как обучить каждую модель?**
**Подготовка данных:**

```python
from sklearn.model_selection 
import train_test_split from sklearn.datasets 
import make_classification 

# Генерация данных 
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42) 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 
```

**Обучение моделей:**
**Logistic Regression:**
```python
from sklearn.linear_model 
import LogisticRegression 

model_lr = LogisticRegression(class_weight='balanced') 
model_lr.fit(X_train, y_train) 
```

**Random Forest:**
```python
from sklearn.ensemble 
import RandomForestClassifier 

model_rf = RandomForestClassifier(class_weight='balanced', random_state=42) 
model_rf.fit(X_train, y_train) 
```

**XGBoost:**
```python
from xgboost 
import XGBClassifier 

model_xgb = XGBClassifier(scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1])) 
model_xgb.fit(X_train, y_train) 
```

## Проведена оценка качества модели

**6. Как провести оценку качества модели?**
**Метрики:**
**Accuracy (Точность):** Доля правильно предсказанных классов.
**Precision, Recall, F1-score:**
**Precision:** Точность предсказаний положительного класса.
**Recall:** Полнота положительного класса.
**F1-score:** Среднее Precision и Recall.
**ROC-AUC (Площадь под ROC-кривой):** Оценка способности модели различать классы.

**Оценка:**
```python
from sklearn.metrics 
import classification_report, roc_auc_score 

# Предсказания 
y_pred_lr = model_lr.predict(X_test) 
y_pred_rf = model_rf.predict(X_test) 
y_pred_xgb = model_xgb.predict(X_test) 

# Метрики print("Logistic Regression:") 
print(classification_report(y_test, y_pred_lr)) 
print("ROC-AUC:", roc_auc_score(y_test, model_lr.predict_proba(X_test)[:, 1])) 
print("Random Forest:") 
print(classification_report(y_test, y_pred_rf)) 
print("ROC-AUC:", roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])) 
print("XGBoost:") 
print(classification_report(y_test, y_pred_xgb)) 
print("ROC-AUC:", roc_auc_score(y_test, model_xgb.predict_proba(X_test)[:, 1]))
```


## Решение автоматически обучает модель

Фраза **"Решение автоматически обучает модель"** означает, что система самостоятельно запускает процесс обучения модели машинного обучения без ручного вмешательства. Это включает в себя автоматизацию всех этапов: сбор данных, предобработку, выбор модели, настройку гиперпараметров, обучение, оценку и обновление модели.

**Как реализовать автоматическое обучение классификационной модели:**
**Автоматизированный сбор и обновление данных**
	Интеграция с источниками данных (API, базы данных, файловые хранилища).
	Регулярное обновление данных с заданной периодичностью или по событию.

**Автоматическая предобработка данных**
	Очистка данных (удаление пропусков, выбросов).
	Кодирование категориальных признаков.
	Масштабирование и нормализация данных.
	Выявление и устранение дисбаланса классов (например, с помощью oversampling или undersampling).

**Выбор модели и обучение**
	Использование AutoML-фреймворков (например, Auto-sklearn, TPOT, H2O.ai) для автоматического подбора модели и гиперпараметров.
	Запуск обучения по расписанию или при обновлении данных.

**Оценка качества модели**
	Автоматическое тестирование на валидационной или тестовой выборке.
	Логирование метрик (точность, полнота, F1-score и т.д.).
	Сравнение новой модели с предыдущей версией.

**Деплой и обновление модели**
	Если новая модель лучше, автоматический деплой в продакшн.
	Контроль версий модели (например, с помощью MLflow).

**Мониторинг и повторное обучение**
	Мониторинг производительности модели на реальных данных.
	Настройка триггеров для переобучения (например, при ухудшении метрик или по расписанию).

**Технологии для автоматизации:**
	**Data Pipeline**: Apache Airflow, Prefect
	**AutoML**: Auto-sklearn, H2O AutoML, Google AutoML
	**Модели и деплой**: MLflow, TensorFlow Serving, Docker, Kubernetes
	**Мониторинг**: Prometheus, Grafana, Evidently AI

Чтобы настроить автоматическое обучение моделей **Random Forest**, **Logistic Regression** и **Gradient Boosting** с данными из **PostgreSQL**, можно реализовать следующую архитектуру решения:

**1. Инфраструктура и инструменты**
	**PostgreSQL** — хранилище данных.
	**Python** + библиотеки: pandas, scikit-learn, xgboost/lightgbm, sqlalchemy.
	**Airflow** или **Prefect** — для автоматизации процессов.
	**MLflow** — для отслеживания экспериментов и деплоя моделей.
	**Docker/Kubernetes** — для контейнеризации и масштабирования.

**2. Пошаговая настройка**
**Шаг 1: Подключение к базе PostgreSQL**
```python
from sqlalchemy import create_engine 
import pandas as pd 

# Параметры подключения 
db_config = { 'host': 'localhost', 'port': '5432', 'database': 'your_database', 'user': 'your_user', 'password': 'your_password' } 

# Создание подключения 
engine = create_engine(f"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}") 

# Загрузка данных 
query = "SELECT * FROM your_table" 
df = pd.read_sql(query, engine) 
```

**Шаг 2: Предобработка данных**
```python
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 

# Разделение признаков и целевой переменной 
X = df.drop('target', axis=1) y = df['target'] 

# Обучающая и тестовая выборки 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

# Масштабирование (для Logistic Regression) 
scaler = StandardScaler() 
X_train_scaled = scaler.fit_transform(X_train) 
X_test_scaled = scaler.transform(X_test) 
```

**Шаг 3: Обучение моделей**
```python
from sklearn.ensemble import RandomForestClassifier 
from sklearn.linear_model import LogisticRegression 
from xgboost import XGBClassifier 
from sklearn.metrics import classification_report 

# Random Forest 
rf_model = RandomForestClassifier(random_state=42) 
rf_model.fit(X_train, y_train) 
rf_preds = rf_model.predict(X_test) 

# Logistic Regression 
lr_model = LogisticRegression(random_state=42) 
lr_model.fit(X_train_scaled, y_train) 
lr_preds = lr_model.predict(X_test_scaled) 

# Gradient Boosting (XGBoost) 
gb_model = XGBClassifier(random_state=42) 
gb_model.fit(X_train, y_train) 
gb_preds = gb_model.predict(X_test) 

# Оценка моделей 
print("Random Forest:\n", classification_report(y_test, rf_preds)) 
print("Logistic Regression:\n", classification_report(y_test, lr_preds)) 
print("Gradient Boosting:\n", classification_report(y_test, gb_preds)) 
```

**Шаг 4: Логирование и отслеживание моделей (MLflow)**
```python
import mlflow import mlflow.sklearn 

mlflow.set_tracking_uri("http://localhost:5000") 

# Адрес MLflow сервера 
mlflow.set_experiment("Model_Training") with mlflow.start_run(): 
	mlflow.sklearn.log_model(rf_model, "random_forest_model") 
	mlflow.log_metric("rf_accuracy", rf_model.score(X_test, y_test)) 
	mlflow.sklearn.log_model(lr_model, "logistic_regression_model") 
	mlflow.log_metric("lr_accuracy", lr_model.score(X_test_scaled, y_test)) 
	mlflow.sklearn.log_model(gb_model, "gradient_boosting_model") 
	mlflow.log_metric("gb_accuracy", gb_model.score(X_test, y_test)) 
```

**Шаг 5: Автоматизация с Airflow**
Создаём DAG в Airflow для автоматического обучения:
```python
from airflow import DAG 
from airflow.operators.python import PythonOperator 
from datetime import datetime 

def train_models(): 
	# Вставить сюда код подключения к БД, предобработки и обучения моделей pass 
	default_args = { 
		'owner': 'airflow', 'start_date': datetime(2024, 1, 1), 'retries': 1, 
	} with DAG(
		'model_training_pipeline', default_args=default_args, 
		schedule_interval='@daily', catchup=False
	) as dag: train_task = PythonOperator( 
		task_id='train_models', 
		python_callable=train_models 
	) 
	train_task 
```

**Шаг 6: Мониторинг и автоматическое переобучение**
	Использовать триггеры на обновление данных в PostgreSQL.
	Проверять метрики в MLflow или логах.
	Автоматически запускать DAG при ухудшении метрик или обновлении данных.

**Итоговая схема работы**
	**Данные** загружаются из **PostgreSQL**.
	Выполняется **предобработка**.
	Обучаются модели: **Random Forest**, **Logistic Regression**, **Gradient Boosting**.
	**MLflow** фиксирует результаты.
	**Airflow** запускает процесс автоматически.
	В случае ухудшения качества — выполняется **переобучение**.


## Для обучения модели в реальном времени используются данные из базы данных

Выше в пункте все есть.

## Полученная модель автоматически сохраняется в директории

Для реализации логики автоматического обучения, дообучения и замены моделей при улучшении метрик можно использовать следующую структуру:

**Архитектура решения**
- **Обучение и сохранение модели при первом запуске.**
- **Дообучение модели по триггеру.**
- **Сравнение метрик старой и новой модели.**
- **Замена старой модели или сохранение новой как дополнительной.**

**Структура директорий**
models/ 
	├── RandomForest/ │ 
					├── best_model.pkl │ 
					└── archive/ 
	├── LogisticRegression/ │ 
					├── best_model.pkl │ 
					└── archive/ 
	└── GradientBoosting/ 
					├── best_model.pkl 
					└── archive/ 

**Пошаговая реализация**
**1. Импорты и подключение**
```python
import os import joblib 
from sklearn.metrics import accuracy_score 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.linear_model import LogisticRegression 
from xgboost import XGBClassifier 
```

**2. Пути для сохранения моделей**
```python
MODEL_DIR = "models" 
MODELS = { 
	"RandomForest": RandomForestClassifier(random_state=42), 
	"LogisticRegression": LogisticRegression(random_state=42, max_iter=1000), 
	"GradientBoosting": XGBClassifier(random_state=42) 
} 
```

**3. Функция для сохранения модели**
```python
def save_model(model, model_name, version="best"): 
	path = os.path.join(MODEL_DIR, model_name) 
	os.makedirs(path, exist_ok=True) 
	
	if version == "best": 
		file_path = os.path.join(path, "best_model.pkl") 
	else: 
		archive_path = os.path.join(path, "archive") 
	
	os.makedirs(archive_path, exist_ok=True) 
	file_path = os.path.join(archive_path, f"model_{version}.pkl") 
	joblib.dump(model, file_path) 
	
	print(f"Модель {model_name} сохранена в {file_path}") 
```

**4. Функция загрузки существующей модели**
```python
def load_model(model_name): 
	path = os.path.join(MODEL_DIR, model_name, "best_model.pkl") 	
	if os.path.exists(path): 
		return joblib.load(path) 
	else:
		return None 
```

**5. Функция для сравнения моделей и обновления**
```python
def compare_and_update_model(model, model_name, X_test, y_test): 
	new_preds = model.predict(X_test) 
	new_score = accuracy_score(y_test, new_preds) 
	old_model = load_model(model_name) 
	
	if old_model: 
		old_preds = old_model.predict(X_test) 
		old_score = accuracy_score(y_test, old_preds) 
		
		if new_score > old_score: 
			save_model(model, model_name, version="best") 
			print(f"Новая модель {model_name} лучше старой. Модель обновлена.") 
		else: 
			version = len(
				os.listdir(os.path.join(MODEL_DIR, model_name, "archive"))) + 1 save_model(model, model_name, version=str(version)) 
				print(f"Новая модель {model_name} хуже. Сохранена в архив.") 
	else: 
		save_model(model, model_name, version="best") 
		print(f"Первая версия модели {model_name} сохранена.") 
```

**6. Основная функция обучения и дообучения**
```python
from sklearn.model_selection import train_test_split 
from sklearn.datasets import load_breast_cancer 

def train_and_update_models(): 
	# Пример данных (заменить на подключение к PostgreSQL) 
	data = load_breast_cancer() 
	
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) 
	
	for model_name, model in MODELS.items(): 
		print(f"\nОбучение модели: {model_name}") 
		model.fit(X_train, y_train) 
		compare_and_update_model(model, model_name, X_test, y_test) 
		
train_and_update_models() 
```

**Как это работает:**
**Первое обучение:**
	Если модели ещё нет, она сохраняется как **best_model.pkl**.

**Дообучение:**
	При запуске (например, по триггеру) обучается новая модель.
	Сравнивается с текущей сохранённой моделью.
	Если новая лучше — заменяет старую.
	Если хуже — сохраняется в **archive/**.

**Триггеры для дообучения:**
	**Расписание:** Использовать Airflow или Cron.
	**По обновлению данных:** Настроить слушателя изменений в PostgreSQL.
	**По ухудшению метрик:** Мониторить производительность в продакшене.

## Проведён сравнительный анализ исходной и дообученной моделей

Выше в пункте есть 

## Рассмотрено не менее трёх алгоритмов регрессии

**1. Три алгоритма регрессии**
**Линейная регрессия (Linear Regression)**
	Простая модель, предполагающая линейную зависимость между признаками и целевой переменной. Подходит для задач с линейными взаимосвязями.

**Дерево решений для регрессии (Decision Tree Regressor)**
	Нелинейная модель, делящая данные на подмножества с помощью условий. Удобна для работы с данными с разветвлённой структурой и взаимодействиями признаков.

**Градиентный бустинг для регрессии (Gradient Boosting Regressor)**
	Сложная ансамблевая модель, которая последовательно строит слабые модели (обычно деревья решений), минимизируя ошибку. Подходит для сложных зависимостей и обеспечивает высокую точность.

## Выбор регрессионной модели обоснован

**Как подобрать регрессионную модель под задачу**
**Анализ данных**
	Проверить линейность зависимости.
	Оценить наличие выбросов, пропусков и корреляцию признаков.

**Размер и сложность данных**
	Малый объём данных → простые модели (линейная регрессия, дерево решений).
	Большой объём и сложные зависимости → сложные модели (градиентный бустинг, нейронные сети).

**Интерпретируемость**
	Требуется объяснение модели → линейная регрессия или дерево решений.
	Нужна высокая точность → ансамбли (градиентный бустинг, случайный лес).

**Оценка производительности**
	Использовать кросс-валидацию для сравнения моделей.
	Оценивать метрики качества (MAE, RMSE, R²).

## Выполнен прогноз загруженности станций

**Пример прогноза загруженности станций с помощью трёх моделей**
```python
import pandas as pd 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
from sklearn.tree import DecisionTreeRegressor 
from sklearn.ensemble import GradientBoostingRegressor 
from sklearn.metrics import mean_squared_error 

# Загрузка данных (пример) 
df = pd.read_csv('station_load.csv') 

# Данные по загруженности станций 
# Предобработка 
X = df.drop('load', axis=1) 
# Признаки 
y = df['load'] 
# Целевая переменная 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

# Линейная регрессия 
lr_model = LinearRegression().fit(X_train, y_train) 
lr_preds = lr_model.predict(X_test) 

# Дерево решений 
dt_model = DecisionTreeRegressor(random_state=42).fit(X_train, y_train) 
dt_preds = dt_model.predict(X_test) 

# Градиентный бустинг 
gb_model = GradientBoostingRegressor(random_state=42).fit(X_train, y_train) 
gb_preds = gb_model.predict(X_test) 

# Оценка моделей 
def evaluate(y_true, y_pred, model_name): 
	rmse = mean_squared_error(y_true, y_pred, squared=False) 
	print(f'{model_name} RMSE: {rmse:.2f}') evaluate(y_test, lr_preds, "Линейная регрессия") evaluate(y_test, dt_preds, "Дерево решений") evaluate(y_test, gb_preds, "Градиентный бустинг") 
```

## Прогнозирование реализовано минимум на два года вперёд

**Прогноз на два года вперёд**
Для прогноза на два года вперёд нужно учесть временные зависимости. Подойдут модели:
	**SARIMA** — для сезонных временных рядов.
	**Gradient Boosting** или **Random Forest** — с созданием лаговых признаков.
	**Prophet** — от Meta, специально для временных рядов.

**Пример с градиентным бустингом:**
```python
import numpy as np 

# Создание лаговых признаков (зависимость от прошлых значений) 
df['lag_1'] = df['load'].shift(1) 
df['lag_7'] = df['load'].shift(7) 
df = df.dropna() X = df[['lag_1', 'lag_7']] 
y = df['load'] 

# Обучение 
gb_model = GradientBoostingRegressor().fit(X, y) 

# Прогноз на 730 дней (2 года) 
future_load = [] 
last_values = df['load'].iloc[-7:].tolist() 

for _ in range(730): 
	X_future = np.array([last_values[-1], last_values[-7]]).reshape(1, -1) 
	pred = gb_model.predict(X_future)[0] 
	future_load.append(pred) 
	last_values.append(pred) 
```

## Присутствует визуализация результатов прогнозирования

**5. Визуализация прогноза**
```python
import matplotlib.pyplot as plt 

# Визуализация прогноза 
plt.figure(figsize=(12, 6)) 
plt.plot(range(len(df['load'])), df['load'], label='Исторические данные') 
plt.plot(range(len(df['load']), len(df['load']) + 730), future_load, label='Прогноз на 2 года', color='red') 
plt.xlabel('Дни') 
plt.ylabel('Загруженность станции') 
plt.title('Прогноз загруженности станции на 2 года') 
plt.legend() 
plt.grid(True) 
plt.show()
```

## Проведена оценка качества модели

ищи в пункте **Выполнен прогноз загруженности станций**

# Разработка программного продукта



